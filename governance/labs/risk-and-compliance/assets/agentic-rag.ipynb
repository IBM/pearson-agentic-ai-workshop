{"metadata": {"kernelspec": {"display_name": "Python 3.11", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Evaluating Answer Quality and Retrieval Quality of Agentic RAG system", "metadata": {"id": "43c19b29-f106-4111-ae53-f4c728a3ecb1"}}, {"cell_type": "markdown", "source": "This notebook demonstrates the following:\n\n* Create an Agent with two tools.\n* One tool is, for a given question, retrieve the context from EU AI Act summary document from https://artificialintelligenceact.eu/high-level-summary/\n* Another tool is, for a given question, retrieve the context from HR FAQs.\n* Against the Agent, run couple of questions related to EU AI Act and HR FAQs using the system prompt and human prompt as described in the notebook.\n* As part of this process, collect the following details - question, context from either EU AI Act related tool or from FAQ related tool, and the respective answers.\n* Then, create a Detached Prompt Template using the combination of the system prompt + human prompt.\n* Use Mistral model as the RAG LLM-as-a-Judge evaluator for evaluating RAG metrics.\n* Log and evaluate the metrics.\n* And visualise the metrics via., watsonx.governance UI.", "metadata": {"id": "39dc54d3-5342-4cd9-8379-f3983dbbdf50"}}, {"cell_type": "markdown", "source": "## Setup <a name=\"settingup\"></a>", "metadata": {"id": "1afb655f-03dd-4da1-a4c9-ccbfa542a645"}}, {"cell_type": "markdown", "source": "### Install the necessary packages", "metadata": {}}, {"cell_type": "code", "source": "!pip install -U ibm-watson-openscale | tail -n 1\n!pip install --upgrade ibm-watsonx-ai | tail -n 1\n!pip install langchain | tail -n 1\n!pip install langchain-ibm | tail -n 1\n!pip install langchain-community | tail -n 1\n!pip install ibm_watson_machine_learning | tail -n 1\n!pip install chromadb | tail -n 1\n!pip install tiktoken | tail -n 1\n!pip install --upgrade ibm-aigov-facts-client | tail -n 1", "metadata": {"id": "1075d4fc-cf23-4c75-b6cf-c97080b52846"}, "outputs": [{"name": "stdout", "text": "^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: typing_extensions>=4.5 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai) (4.12.2)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.1.14->langchain-ibm) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from lomond->ibm_watson_machine_learning) (1.16.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.1->ibm-aigov-facts-client) (0.4.8)\n", "output_type": "stream"}], "execution_count": 96}, {"cell_type": "markdown", "source": "### Restart the kernel", "metadata": {"id": "a4012b1e-b406-4f2b-abb9-8839a17e3e02"}}, {"cell_type": "code", "source": "import warnings\nwarnings.filterwarnings(\"ignore\")", "metadata": {"id": "7d29ffd6-d07b-4eec-a975-2e28419af5ae"}, "outputs": [], "execution_count": 97}, {"cell_type": "markdown", "source": "## Imports <a name=\"Necessary Imports\"></a>", "metadata": {"id": "2b22fc40-45d6-4557-8039-6e8536c8dfb8"}}, {"cell_type": "code", "source": "# imports\nimport os\n\n#from dotenv import load_dotenv\nfrom langchain_ibm import WatsonxEmbeddings, WatsonxLLM\nfrom langchain.vectorstores import Chroma\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain.prompts import PromptTemplate\nfrom langchain.tools import tool\nfrom langchain.tools.render import render_text_description_and_args\nfrom langchain.agents.output_parsers import JSONAgentOutputParser\nfrom langchain.agents.format_scratchpad import format_log_to_str\nfrom langchain.agents import AgentExecutor\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_core.runnables import RunnablePassthrough\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes", "metadata": {"id": "ffdf8565-db6f-4492-9cd0-55e40723b79f"}, "outputs": [], "execution_count": 98}, {"cell_type": "markdown", "source": "### Configure your credentials", "metadata": {"id": "a1c1166f-6bcb-4009-aaf0-89e12e236add"}}, {"cell_type": "code", "source": "IAM_URL = \"https://iam.cloud.ibm.com\"\nDATAPLATFORM_URL = \"https://api.dataplatform.cloud.ibm.com\"\nFACTSHEET_URL = \"https://dataplatform.cloud.ibm.com\"\nSERVICE_URL = \"https://aiopenscale.cloud.ibm.com\"\nCLOUD_API_KEY = \"*********\"\n\ncredentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": CLOUD_API_KEY,\n}\nCREDENTIALS = credentials\nproject_id = \"***********\"", "metadata": {"id": "d17f26b6-ccca-4714-8576-2308f23bf9a3"}, "outputs": [], "execution_count": 99}, {"cell_type": "markdown", "source": "## watsonx LLM for Prompt Generation <a name=\"Prompt Generation\"></a>", "metadata": {"id": "ebe098e3-5f3d-424d-8a4f-f4d95827280f"}}, {"cell_type": "code", "source": "llm = WatsonxLLM(\n    model_id=\"ibm/granite-3-8b-instruct\", \n    url=credentials.get(\"url\"),\n    apikey=credentials.get(\"apikey\"),\n    project_id=project_id,\n    params={\n        GenParams.DECODING_METHOD: \"greedy\",\n        GenParams.TEMPERATURE: 0,\n        GenParams.MIN_NEW_TOKENS: 5,\n        GenParams.MAX_NEW_TOKENS: 250,\n        GenParams.STOP_SEQUENCES: [\"Human:\", \"Observation\"],\n    },\n)", "metadata": {"id": "6d329d81-442d-47cb-9a65-5bed5a44132e"}, "outputs": [], "execution_count": 100}, {"cell_type": "markdown", "source": "## Slate Model for Embeddings Generation <a name=\"Embeddings Generation\"></a>", "metadata": {"id": "f48349a8-ff7e-4120-938d-c36e66f6dacb"}}, {"cell_type": "code", "source": "def get_embeddings():\n    embeddings = WatsonxEmbeddings(\n        model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,\n        url=credentials[\"url\"],\n        apikey=credentials[\"apikey\"],\n        project_id=project_id,\n    )\n    return embeddings", "metadata": {"id": "51f72c5a-7d99-4a0e-9195-582b778772fc"}, "outputs": [], "execution_count": 101}, {"cell_type": "code", "source": "def get_text_splitter():\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=250, chunk_overlap=0\n    )\n    return text_splitter", "metadata": {"id": "c6dec697-f979-46d5-a431-ea935473304f"}, "outputs": [], "execution_count": 102}, {"cell_type": "markdown", "source": "## Doc URLS <a name=\"Doc URLS\"></a>", "metadata": {"id": "63aa308a-a92a-4721-bd2d-2738c95561b3"}}, {"cell_type": "code", "source": "hr_faq_urls = [\n    'https://github.com/ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/blob/main/data_files/jp-morgan-chase-code-of-conduct-policy.pdf'\n]\n\nai_act_urls = [\n    'https://artificialintelligenceact.eu/high-level-summary/'\n]\n", "metadata": {"id": "29931b66-efcc-4fa4-8830-9e1e39c978e8"}, "outputs": [], "execution_count": 103}, {"cell_type": "markdown", "source": "## Vector Store Retriever against a given doc source<a name=\"Vector Store Retriever\"></a>", "metadata": {"id": "ed0576d0-090c-423c-ae54-58094171ea4b"}}, {"cell_type": "code", "source": "def get_retriever(urls, collection_name):\n    docs = [WebBaseLoader(url).load() for url in urls]\n    docs_list = [item for sublist in docs for item in sublist]\n    text_splitter = get_text_splitter()\n    doc_splits = text_splitter.split_documents(docs_list)\n    vectorstore = Chroma.from_documents(\n        documents=doc_splits,\n        collection_name=collection_name,\n        embedding=get_embeddings(),\n    )\n    retriever = vectorstore.as_retriever()\n    return retriever", "metadata": {"id": "1ca5cc5f-f338-4518-8533-484e0281c1f6"}, "outputs": [], "execution_count": 104}, {"cell_type": "markdown", "source": "## Retriever for FAQs document", "metadata": {"id": "3c92c511-5573-4572-9f5c-bf30d249b8d3"}}, {"cell_type": "code", "source": "hr_faqs_retriever = get_retriever(urls=hr_faq_urls, collection_name='hr_faqs')\nai_act_retriever = get_retriever(urls=ai_act_urls, collection_name='ai_act')", "metadata": {"id": "0ec06b60-09c0-494e-ac92-d4bacfb61561"}, "outputs": [], "execution_count": 105}, {"cell_type": "markdown", "source": "## Agentic AI tools. One wrapping the FAQs and other wrapping the EU AI Act summary document", "metadata": {"id": "2b2db67a-de70-4752-8b2c-ae3ba68d115a"}}, {"cell_type": "code", "source": "\nhr_faqs_context = \"\"\nai_act_context = \"\"\n\n@tool\ndef get_HR_FAQs_Context(question: str):\n    \"\"\"Get context from Hr documents related Frequently asked questions.\"\"\"\n    global hr_faqs_context\n    hr_faqs_context = hr_faqs_retriever.invoke(question)\n    return hr_faqs_context\n\n@tool\ndef get_AI_Act_Summary_Context(question: str):\n    \"\"\"Get context from High-level summary of the AI Act.\"\"\"\n    global ai_act_context\n    ai_act_context = ai_act_retriever.invoke(question)\n    return ai_act_context\n\ntools = [get_AI_Act_Summary_Context, get_HR_FAQs_Context]", "metadata": {"id": "488f9484-1dfe-49a3-820c-30fc0a3b2565"}, "outputs": [], "execution_count": 106}, {"cell_type": "markdown", "source": "## Standard Langchain based System Prompt working as an Generative AI Agent\n\nNext, we will set up a new prompt template to ask multiple questions. This template is more complex. It is referred to as a [structured chat prompt](https://api.python.langchain.com/en/latest/agents/langchain.agents.structured_chat.base.create_structured_chat_agent.html#langchain-agents-structured-chat-base-create-structured-chat-agent) and can be used for creating agents that have multiple tools available. In our case, the tool we are using was defined in Step 6. The structured chat prompt will be made up of a `system_prompt`, a `human_prompt` and our RAG tool. \n\nFirst, we will set up the `system_prompt`. This prompt instructs the agent to print its \"thought process,\" which involves the agent's subtasks, the tools that were used and the final output. This gives us insight into the agent's function calling. The prompt also instructs the agent to return its responses in JSON Blob format.", "metadata": {"id": "fb342e57-55f5-41d8-83dd-92fb78d8b4ef"}}, {"cell_type": "code", "source": "system_prompt = \"\"\"Respond to the human as helpfully and accurately as possible. You have access to the following tools: {tools}\nUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\nValid \"action\" values: \"Final Answer\" or {tool_names}\nProvide only ONE action per $JSON_BLOB, as shown:\"\n```\n{{\n  \"action\": $TOOL_NAME,\n  \"action_input\": $INPUT\n}}\n```\nFollow this format:\nQuestion: input question to answer\nThought: consider previous and subsequent steps\nAction:\n```\n$JSON_BLOB\n```\nObservation: action result\n... (repeat Thought/Action/Observation N times)\nThought: I know what to respond\nAction:\n```\n{{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"Final response to human\"\n}}\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action.\nRespond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\"\"\"", "metadata": {"id": "ec330646-2c8c-424d-9e2c-6eb5a0308cf8"}, "outputs": [], "execution_count": 107}, {"cell_type": "markdown", "source": "## The human prompt\n\nIn the following code, we are establishing the `human_prompt`. This prompt tells the agent to display the user input followed by the intermediate steps taken by the agent as part of the `agent_scratchpad`.", "metadata": {"id": "7ce799d3-146f-457f-8542-9af7d54bd768"}}, {"cell_type": "code", "source": "human_prompt = \"\"\"{input}\n{agent_scratchpad}\n(reminder to always respond in a JSON blob)\"\"\"", "metadata": {"id": "7c29eed5-219b-4b0a-aead-787da37d1cd5"}, "outputs": [], "execution_count": 108}, {"cell_type": "markdown", "source": "Next, we establish the order of our newly defined prompts in the prompt template. We create this new template to feature the `system_prompt` followed by an optional list of messages collected in the agent's memory, if any, and finally, the `human_prompt` which includes both the human input and `agent_scratchpad`.", "metadata": {"id": "5d023856-69b4-40f2-8970-313fc846037f"}}, {"cell_type": "code", "source": "prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        MessagesPlaceholder(\"chat_history\", optional=True),\n        (\"human\", human_prompt),\n    ]\n)", "metadata": {"id": "1ddd3aa3-df2b-4a7b-9ee3-22b308628373"}, "outputs": [], "execution_count": 109}, {"cell_type": "markdown", "source": "Now, let's finalize our prompt template by adding the tool names, descriptions and arguments using a [partial prompt template](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/partial/). This allows the agent to access the information pertaining to each tool including the intended use cases and also means we can add and remove tools without altering our entire prompt template.", "metadata": {"id": "267a90df-211b-48a0-a77d-7b87bcbbc8bb"}}, {"cell_type": "code", "source": "prompt = prompt.partial(\n    tools=render_text_description_and_args(list(tools)),\n    tool_names=\", \".join([t.name for t in tools]),\n)", "metadata": {"id": "507e24be-78a9-46c3-b4c3-0ece533c36fb"}, "outputs": [], "execution_count": 110}, {"cell_type": "markdown", "source": "## Set up the agent's memory and chain\n\nAn important feature of AI agents is their memory. Agents are able to store past conversations and past findings in their memory to improve the accuracy and relevance of their responses going forward. In our case, we will use LangChain's `ConversationBufferMemory()` as a means of memory storage. ", "metadata": {"id": "0ad9f708-7de3-4d1a-b7d1-217d0a072b2e"}}, {"cell_type": "code", "source": "memory = ConversationBufferMemory()", "metadata": {"id": "13b1eae8-f20c-4cd6-9d2c-83ff8d540dd3"}, "outputs": [], "execution_count": 111}, {"cell_type": "code", "source": "prompt_input = prompt.messages[0].prompt.template + '\\n\\n' + prompt.messages[2].prompt.template\nprint(prompt_input)", "metadata": {"id": "b3fa682a-a44f-461c-8e41-8a47d937c3ac"}, "outputs": [{"name": "stdout", "text": "Respond to the human as helpfully and accurately as possible. You have access to the following tools: {tools}\nUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\nValid \"action\" values: \"Final Answer\" or {tool_names}\nProvide only ONE action per $JSON_BLOB, as shown:\"\n```\n{{\n  \"action\": $TOOL_NAME,\n  \"action_input\": $INPUT\n}}\n```\nFollow this format:\nQuestion: input question to answer\nThought: consider previous and subsequent steps\nAction:\n```\n$JSON_BLOB\n```\nObservation: action result\n... (repeat Thought/Action/Observation N times)\nThought: I know what to respond\nAction:\n```\n{{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"Final response to human\"\n}}\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action.\nRespond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\n\n{input}\n{agent_scratchpad}\n(reminder to always respond in a JSON blob)\n", "output_type": "stream"}], "execution_count": 112}, {"cell_type": "markdown", "source": "And now we can set up a chain with our agent's scratchpad, memory, prompt and the LLM. The AgentExecutor class is used to execute the agent. It takes the agent, its tools, error handling approach, verbose parameter and memory as parameters.", "metadata": {"id": "03fb4493-8a41-4b85-8d45-c4f5a369ebd2"}}, {"cell_type": "code", "source": "chain = (\n    RunnablePassthrough.assign(\n        agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n        chat_history=lambda x: memory.chat_memory.messages,\n    )\n    | prompt\n    | llm\n    | JSONAgentOutputParser()\n)\n\nagent_executor = AgentExecutor(\n    agent=chain, tools=tools, handle_parsing_errors=True, verbose=True, memory=memory\n)", "metadata": {"id": "943aa7cb-cc68-4424-ba3e-68257cd43a44"}, "outputs": [], "execution_count": 113}, {"cell_type": "markdown", "source": "## Utility method to construct the context as retrieved from respective vector store", "metadata": {"id": "07acd16c-b406-437e-af8d-48746d75585e"}}, {"cell_type": "code", "source": "def construct_context(relevant_context):\n    context_str = ''\n    for doc in relevant_context:\n        context_str = context_str + doc.page_content + '\\n\\n'\n    return context_str    ", "metadata": {"id": "03be8021-6e1a-4cda-8ddb-7db7c4266766"}, "outputs": [], "execution_count": 114}, {"cell_type": "code", "source": "# Store the complete the context\ncomplete_content = []", "metadata": {"id": "3f832cbc-a365-4f31-8461-7ccbb640edf9"}, "outputs": [], "execution_count": 115}, {"cell_type": "markdown", "source": "## Generate responses with the agentic RAG system\n\nWe are now able to ask the agent questions. Recall the agent's previous inability to provide us with information pertaining to the 2024 US Open. Now that the agent has its RAG tool available to use, let's try asking the same questions again. ", "metadata": {"id": "a919b009-2a7c-4bd5-96cc-f2ec8009073d"}}, {"cell_type": "markdown", "source": "### Question 1. Related to FAQs", "metadata": {"id": "908321ac-7b7c-4a0e-8f10-ffa73d06cb53"}}, {"cell_type": "code", "source": "response = agent_executor.invoke({\"input\": \"What is compliance with law and firm policies for HR process?\"})", "metadata": {"id": "438803ec-5071-40cd-9763-80c0cd0add11"}, "outputs": [{"name": "stdout", "text": "\n\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\n\nQuestion: What is compliance with law and firm policies for HR process?\nThought: The user is asking about the compliance requirements for HR processes. This involves both legal compliance and adherence to the firm's policies. I need to provide information from both the AI Act summary and HR FAQs.\n\nAction:\n```\n{\n  \"action\": \"get_AI_Act_Summary_Context\",\n  \"action_input\": \"What are the compliance requirements for HR processes according to the AI Act?\"\n}\n```\n\nObservation\u001b[0m\u001b[36;1m\u001b[1;3m[Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='All GPAI model providers may demonstrate compliance with their obligations if they voluntarily adhere to a code of practice until European harmonised standards are published, compliance with which will lead to a presumption of conformity. Providers that don\u2019t adhere to codes of practice must demonstrate alternative adequate means of compliance for Commission approval.\\nCodes of practice\\n\\nWill account for international approaches.\\nWill cover but not necessarily limited to the above obligations, particularly the relevant information to include in technical documentation for authorities and downstream providers, identification of the type and nature of systemic risks and their sources, and the modalities of risk management accounting for specific challenges in addressing risks due to the way they may emerge and materialise throughout the value chain.\\nAI Office may invite GPAI model providers, relevant national competent authorities to participate in drawing up the codes, while civil society, industry, academia, downstream providers and independent experts may support the process.\\n\\n\\nGovernance\\nHow will the AI Act be implemented?'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='All GPAI model providers may demonstrate compliance with their obligations if they voluntarily adhere to a code of practice until European harmonised standards are published, compliance with which will lead to a presumption of conformity. Providers that don\u2019t adhere to codes of practice must demonstrate alternative adequate means of compliance for Commission approval.\\nCodes of practice\\n\\nWill account for international approaches.\\nWill cover but not necessarily limited to the above obligations, particularly the relevant information to include in technical documentation for authorities and downstream providers, identification of the type and nature of systemic risks and their sources, and the modalities of risk management accounting for specific challenges in addressing risks due to the way they may emerge and materialise throughout the value chain.\\nAI Office may invite GPAI model providers, relevant national competent authorities to participate in drawing up the codes, while civil society, industry, academia, downstream providers and independent experts may support the process.\\n\\n\\nGovernance\\nHow will the AI Act be implemented?'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='About us\\nNewsletter\\nEN\\n\\nFR\\nDE\\nES\\n\\n\\n \\n\\n\\nSelect Page\\n\\n\\n  \\n \\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHigh-level summary of the AI Act27 Feb, 2024\\n\\n\\n\\nUpdated on 30 May in accordance with the Corrigendum version of the AI Act.\\nIn this article we provide you with a high-level summary of the AI Act, selecting the parts which are most likely to be relevant to you regardless of who you are. We provide links to the original document where relevant so that you can always reference the Act text.\\nTo explore the full text of the AI Act yourself, use our AI Act Explorer. Alternatively, if you want to know which parts of the text are most relevant to you, use our Compliance Checker.\\nView this content as a PDF\\n\\nFour-point summary\\nThe AI Act classifies AI according to its risk:'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='About us\\nNewsletter\\nEN\\n\\nFR\\nDE\\nES\\n\\n\\n \\n\\n\\nSelect Page\\n\\n\\n  \\n \\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHigh-level summary of the AI Act27 Feb, 2024\\n\\n\\n\\nUpdated on 30 May in accordance with the Corrigendum version of the AI Act.\\nIn this article we provide you with a high-level summary of the AI Act, selecting the parts which are most likely to be relevant to you regardless of who you are. We provide links to the original document where relevant so that you can always reference the Act text.\\nTo explore the full text of the AI Act yourself, use our AI Act Explorer. Alternatively, if you want to know which parts of the text are most relevant to you, use our Compliance Checker.\\nView this content as a PDF\\n\\nFour-point summary\\nThe AI Act classifies AI according to its risk:')]\u001b[0m\u001b[32;1m\u001b[1;3m\n\nAction:\n```\n{\n  \"action\": \"get_HR_FAQs_Context\",\n  \"action_input\": \"What are the compliance requirements for HR processes according to the firm's policies?\"\n}\n```\n\nObservation\u001b[0m\u001b[33;1m\u001b[1;3m[Document(metadata={'description': 'This repository contains an HR Policy Query Resolution system using Retrieval-Augmented Generation (RAG). It leverages a 4-bit quantized Mistral-7B-Instruct-v0.2 LLM and JP Morgan Chase\u2019s publicly available Code of Conduct documents to generate accurate, contextually relevant responses for HR policy queries. - HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/data_files/jp-morgan-chase-code-of-conduct-policy.pdf at main \u00b7 ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG', 'language': 'en', 'source': 'https://github.com/ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/blob/main/data_files/jp-morgan-chase-code-of-conduct-policy.pdf', 'title': 'HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/data_files/jp-morgan-chase-code-of-conduct-policy.pdf at main \u00b7 ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG \u00b7 GitHub'}, page_content='HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/data_files/jp-morgan-chase-code-of-conduct-policy.pdf at main \u00b7 ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG \u00b7 GitHub\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Sign in\\n          \\n\\n\\n \\n\\n\\nAppearance settings\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Product'), Document(metadata={'description': 'This repository contains an HR Policy Query Resolution system using Retrieval-Augmented Generation (RAG). It leverages a 4-bit quantized Mistral-7B-Instruct-v0.2 LLM and JP Morgan Chase\u2019s publicly available Code of Conduct documents to generate accurate, contextually relevant responses for HR policy queries. - HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/data_files/jp-morgan-chase-code-of-conduct-policy.pdf at main \u00b7 ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG', 'language': 'en', 'source': 'https://github.com/ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/blob/main/data_files/jp-morgan-chase-code-of-conduct-policy.pdf', 'title': 'HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/data_files/jp-morgan-chase-code-of-conduct-policy.pdf at main \u00b7 ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG \u00b7 GitHub'}, page_content='HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/data_files/jp-morgan-chase-code-of-conduct-policy.pdf at main \u00b7 ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG \u00b7 GitHub\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNavigation Menu\\n\\nToggle navigation\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Sign in\\n          \\n\\n\\n \\n\\n\\nAppearance settings\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        Product'), Document(metadata={'description': 'This repository contains an HR Policy Query Resolution system using Retrieval-Augmented Generation (RAG). It leverages a 4-bit quantized Mistral-7B-Instruct-v0.2 LLM and JP Morgan Chase\u2019s publicly available Code of Conduct documents to generate accurate, contextually relevant responses for HR policy queries. - HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/data_files/jp-morgan-chase-code-of-conduct-policy.pdf at main \u00b7 ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG', 'language': 'en', 'source': 'https://github.com/ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/blob/main/data_files/jp-morgan-chase-code-of-conduct-policy.pdf', 'title': 'HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/data_files/jp-morgan-chase-code-of-conduct-policy.pdf at main \u00b7 ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG \u00b7 GitHub'}, page_content='GitHub Advanced Security\\n          \\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            Actions\\n          \\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Codespaces\\n          \\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            Issues\\n          \\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            Code Review\\n          \\n        Manage code changes'), Document(metadata={'description': 'This repository contains an HR Policy Query Resolution system using Retrieval-Augmented Generation (RAG). It leverages a 4-bit quantized Mistral-7B-Instruct-v0.2 LLM and JP Morgan Chase\u2019s publicly available Code of Conduct documents to generate accurate, contextually relevant responses for HR policy queries. - HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/data_files/jp-morgan-chase-code-of-conduct-policy.pdf at main \u00b7 ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG', 'language': 'en', 'source': 'https://github.com/ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/blob/main/data_files/jp-morgan-chase-code-of-conduct-policy.pdf', 'title': 'HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG/data_files/jp-morgan-chase-code-of-conduct-policy.pdf at main \u00b7 ChaitanyaC22/HR_Policy_Query_Resolution_with_Retrieval_Augmented_Generation_RAG \u00b7 GitHub'}, page_content='GitHub Advanced Security\\n          \\n        Find and fix vulnerabilities\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            Actions\\n          \\n        Automate any workflow\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Codespaces\\n          \\n        Instant dev environments\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            Issues\\n          \\n        Plan and track work\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n            Code Review\\n          \\n        Manage code changes')]\u001b[0m\u001b[32;1m\u001b[1;3m\n\nAction:\n```\n{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"Compliance with law and firm policies for HR processes involves adhering to the AI Act's requirements, such as demonstrating compliance with obligations through voluntary adherence to a code of practice or demonstrating alternative adequate means of compliance. Additionally, HR processes must comply with the firm's policies, which are outlined in the JP Morgan Chase Code of Conduct. These policies cover various aspects of HR management, including recruitment, performance management, and employee relations.\"\n}\n```\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n", "output_type": "stream"}], "execution_count": 116}, {"cell_type": "markdown", "source": "## Capture the output for logging with watsonx.governance", "metadata": {"id": "aa2f6c4a-5c41-44bc-bc73-a7c3e2d1f255"}}, {"cell_type": "code", "source": "prompt_result = {}\nprompt_result['query'] = response['input']\nprompt_result['context'] = construct_context(hr_faqs_context)\nprompt_result['generated_text'] = response['output']\ncomplete_content.append(prompt_result)", "metadata": {"id": "374d0614-392a-4d75-b1d3-89697626b5e3"}, "outputs": [], "execution_count": 117}, {"cell_type": "markdown", "source": "### Question 2. Related to EU AI Act", "metadata": {"id": "97deb0ac-1724-4dc6-99bb-64a8a05aa9fb"}}, {"cell_type": "code", "source": "response = agent_executor.invoke(\n    {\"input\": \"What are considered as Prohibited AI systems?\"}\n)", "metadata": {"id": "0fa52925-d372-49fb-9db1-d0de8788e0e7"}, "outputs": [{"name": "stdout", "text": "\n\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\n\n```\n{\n  \"action\": \"get_AI_Act_Summary_Context\",\n  \"action_input\": \"What are considered as Prohibited AI systems?\"\n}\n```\nObservation\u001b[0m\u001b[36;1m\u001b[1;3m[Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='Users (deployers) of high-risk AI systems have some obligations, though less than providers (developers).\\nThis applies to users located in the EU, and third country users where the AI system\u2019s output is used in the EU.\\n\\nGeneral purpose AI (GPAI):\\n\\nAll GPAI model providers must provide technical documentation, instructions for use, comply with the Copyright Directive, and publish a summary about the content used for training.\\nFree and open licence GPAI model providers only need to comply with copyright and publish the training data summary, unless they present a systemic risk.\\nAll providers of GPAI models that present a systemic risk \u2013 open or closed \u2013 must also conduct model evaluations, adversarial testing, track and report serious incidents and ensure cybersecurity protections.\\n\\n\\n\\nProhibited AI systems (Chapter II, Art. 5)\\nThe following types of AI system are \u2018Prohibited\u2019 according to the AI Act.\\nAI systems:'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='Users (deployers) of high-risk AI systems have some obligations, though less than providers (developers).\\nThis applies to users located in the EU, and third country users where the AI system\u2019s output is used in the EU.\\n\\nGeneral purpose AI (GPAI):\\n\\nAll GPAI model providers must provide technical documentation, instructions for use, comply with the Copyright Directive, and publish a summary about the content used for training.\\nFree and open licence GPAI model providers only need to comply with copyright and publish the training data summary, unless they present a systemic risk.\\nAll providers of GPAI models that present a systemic risk \u2013 open or closed \u2013 must also conduct model evaluations, adversarial testing, track and report serious incidents and ensure cybersecurity protections.\\n\\n\\n\\nProhibited AI systems (Chapter II, Art. 5)\\nThe following types of AI system are \u2018Prohibited\u2019 according to the AI Act.\\nAI systems:'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='used as a safety component or a product covered by EU laws in Annex I AND required to undergo a third-party conformity assessment under those Annex I laws; OR\\nlisted under Annex III use cases (below), except if:\\n\\nthe AI system performs a narrow procedural task;\\nimproves the result of a previously completed human activity;\\ndetects decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment without proper human review; or\\nperforms a preparatory task to an assessment relevant for the purpose of the use cases listed in Annex III.\\n\\n\\nAI systems listed under Annex III are always considered high-risk if it profiles individuals, i.e. automated processing of personal data to assess various aspects of a person\u2019s life, such as work performance, economic situation, health, preferences, interests, reliability, behaviour, location or movement.\\nProviders whose AI system falls under the use cases in Annex III but believes it is not high-risk must document such an assessment before placing it on the market or putting it into service.'), Document(metadata={'language': 'en-US', 'source': 'https://artificialintelligenceact.eu/high-level-summary/', 'title': 'High-level summary of the AI Act | EU Artificial Intelligence Act'}, page_content='used as a safety component or a product covered by EU laws in Annex I AND required to undergo a third-party conformity assessment under those Annex I laws; OR\\nlisted under Annex III use cases (below), except if:\\n\\nthe AI system performs a narrow procedural task;\\nimproves the result of a previously completed human activity;\\ndetects decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment without proper human review; or\\nperforms a preparatory task to an assessment relevant for the purpose of the use cases listed in Annex III.\\n\\n\\nAI systems listed under Annex III are always considered high-risk if it profiles individuals, i.e. automated processing of personal data to assess various aspects of a person\u2019s life, such as work performance, economic situation, health, preferences, interests, reliability, behaviour, location or movement.\\nProviders whose AI system falls under the use cases in Annex III but believes it is not high-risk must document such an assessment before placing it on the market or putting it into service.')]\u001b[0m\u001b[32;1m\u001b[1;3m\nAction:\n```\n{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"Prohibited AI systems, as per the AI Act, include those used as a safety component or a product covered by EU laws in Annex I and required to undergo a third-party conformity assessment under those Annex I laws. Additionally, AI systems listed under Annex III are considered prohibited if they profile individuals, i.e., automated processing of personal data to assess various aspects of a person's life, such as work performance, economic situation, health, preferences, interests, reliability, behavior, location, or movement. However, there are exceptions for narrow procedural tasks, improving the result of a previously completed human activity, detecting decision-making patterns or deviations from prior decision-making patterns without replacing or influencing the previously completed human assessment without proper human review, and performing preparatory tasks to an assessment relevant for the purpose of the use cases listed in Annex III.\"\n}\n```\nObservation\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n", "output_type": "stream"}], "execution_count": 118}, {"cell_type": "code", "source": "prompt_result = {}\nprompt_result['query'] = response['input']\nprompt_result['context'] = construct_context(ai_act_context)\nprompt_result['generated_text'] = response['output']\ncomplete_content.append(prompt_result)", "metadata": {"id": "f58754cb-f602-4d1d-aadb-67593bdeb9fd"}, "outputs": [], "execution_count": 119}, {"cell_type": "markdown", "source": "### Question 3. Related to EU AI Act", "metadata": {"id": "03671fad-ede9-4d6d-8ae9-200735aab0e5"}}, {"cell_type": "code", "source": "response = agent_executor.invoke(\n    {\"input\": \"What is a dynamic and thriving space?\"}\n)", "metadata": {"id": "c70f4d78-e5bb-4d9d-97c4-4a1ca47acc57"}, "outputs": [{"name": "stdout", "text": "\n\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\n\nAI:\n```\n{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"A dynamic and thriving space refers to an environment that is constantly evolving, adapting, and growing. It is characterized by innovation, creativity, and continuous improvement. In the context of HR processes, this could mean a workplace that encourages employee development, fosters collaboration, and embraces change. However, the term 'dynamic and thriving space' is not explicitly defined in the AI Act or the JP Morgan Chase Code of Conduct.\"\n}\n```\nObservation\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n", "output_type": "stream"}], "execution_count": 120}, {"cell_type": "code", "source": "prompt_result = {}\nprompt_result['query'] = response['input']\nprompt_result['context'] = construct_context(ai_act_context)\nprompt_result['generated_text'] = response['output']\ncomplete_content.append(prompt_result)", "metadata": {"id": "3f18e5cb-9d2b-4c89-bff9-f02673c20cbf"}, "outputs": [], "execution_count": 121}, {"cell_type": "markdown", "source": "### Question 4. Related to HR FAQs", "metadata": {"id": "76441842-2ef4-4a21-bb72-3a58dc3f85fe"}}, {"cell_type": "code", "source": "response = agent_executor.invoke(\n    {\"input\": \"How is work/life balance?\"}\n)", "metadata": {"id": "44c97062-61c7-4709-a7c9-dc5e9d0c8488"}, "outputs": [{"name": "stdout", "text": "\n\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\n```\n{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"Work/life balance is a concept that refers to the equilibrium between an individual's professional responsibilities and personal life. It is not explicitly defined in the AI Act or the JP Morgan Chase Code of Conduct. However, JP Morgan Chase promotes work/life balance through various initiatives, such as flexible work arrangements, paid time off, and employee wellness programs. These initiatives aim to support employees in managing their professional and personal commitments effectively.\"\n}\n```\nObservation\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n", "output_type": "stream"}], "execution_count": 122}, {"cell_type": "code", "source": "prompt_result = {}\nprompt_result['query'] = response['input']\nprompt_result['context'] = construct_context(hr_faqs_context)\nprompt_result['generated_text'] = response['output']\ncomplete_content.append(prompt_result)", "metadata": {"id": "e59856d8-9b97-4966-ad71-224ca19849ef"}, "outputs": [], "execution_count": 123}, {"cell_type": "code", "source": "import pandas as pd\nllm_data = pd.DataFrame(complete_content)", "metadata": {"id": "b14d579a-4fea-4ef0-af81-f00bec6ea0a0"}, "outputs": [], "execution_count": 124}, {"cell_type": "code", "source": "llm_data['reference'] = llm_data['generated_text']", "metadata": {"id": "16a119f4-7bf9-485c-be6e-823d9530152c"}, "outputs": [], "execution_count": 125}, {"cell_type": "code", "source": "llm_data.head()", "metadata": {"id": "35735ae0-d03b-41cd-97f6-e04c6ee39fdf"}, "outputs": [{"execution_count": 126, "output_type": "execute_result", "data": {"text/plain": "                                               query  \\\n0  What is compliance with law and firm policies ...   \n1      What are considered as Prohibited AI systems?   \n2              What is a dynamic and thriving space?   \n3                          How is work/life balance?   \n\n                                             context  \\\n0  HR_Policy_Query_Resolution_with_Retrieval_Augm...   \n1  Users (deployers) of high-risk AI systems have...   \n2  Users (deployers) of high-risk AI systems have...   \n3  HR_Policy_Query_Resolution_with_Retrieval_Augm...   \n\n                                      generated_text  \\\n0  Compliance with law and firm policies for HR p...   \n1  Prohibited AI systems, as per the AI Act, incl...   \n2  A dynamic and thriving space refers to an envi...   \n3  Work/life balance is a concept that refers to ...   \n\n                                           reference  \n0  Compliance with law and firm policies for HR p...  \n1  Prohibited AI systems, as per the AI Act, incl...  \n2  A dynamic and thriving space refers to an envi...  \n3  Work/life balance is a concept that refers to ...  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query</th>\n      <th>context</th>\n      <th>generated_text</th>\n      <th>reference</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is compliance with law and firm policies ...</td>\n      <td>HR_Policy_Query_Resolution_with_Retrieval_Augm...</td>\n      <td>Compliance with law and firm policies for HR p...</td>\n      <td>Compliance with law and firm policies for HR p...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What are considered as Prohibited AI systems?</td>\n      <td>Users (deployers) of high-risk AI systems have...</td>\n      <td>Prohibited AI systems, as per the AI Act, incl...</td>\n      <td>Prohibited AI systems, as per the AI Act, incl...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What is a dynamic and thriving space?</td>\n      <td>Users (deployers) of high-risk AI systems have...</td>\n      <td>A dynamic and thriving space refers to an envi...</td>\n      <td>A dynamic and thriving space refers to an envi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How is work/life balance?</td>\n      <td>HR_Policy_Query_Resolution_with_Retrieval_Augm...</td>\n      <td>Work/life balance is a concept that refers to ...</td>\n      <td>Work/life balance is a concept that refers to ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}], "execution_count": 126}, {"cell_type": "markdown", "source": "# Computing Answer Quality and Retrieval Quality Metrics using IBM watsonx.governance for RAG task\n\nThis notebook demonstrates the creation of a Retrieval Augumented Generation (RAG) pattern using watsonx.ai and computations of reference-free Answer Quality metrics, such as **Faithfulness**, **Answer relevance**, **Unsuccessful requests**, Content Analysis metrics such as **Coverage**, **Density**, **Abstractness** and Retrieval Quality Metrics such as **Context Relevance**, **Retrieval Precision**, **Average Precision**, **Reciprocal Rank**, **Hit Rate** and **Normalized Discounted Cumulative Gain** for the RAG task type. It also identifies the source attribution using watsonx.governance.\n\n- **Faithfulness** measures how faithful the model output or generated text is to the context sent to the LLM input. The faithfulness score is a value between 0 and 1. A value closer to 1 indicates that the output is more faithful - or grounded - and less hallucinated. A value closer to 0 indicates that the output is less faithful and more hallucinated.\n\n- **Answer relevance** measures how relevant the answer or generated text is to the question. This is one of the ways to determine the quality of your model. The answer relevance score is a value between 0 and 1. A value closer to 1 indicates that the answer is more relevant to the given question. A value closer to 0 indicates that the answer is less relevant to the question.\n\n- **Unsuccessful requests** measures the ratio of questions answered unsuccessfully out of the total number of questions. The unsuccessful requests score is a value between 0 and 1. A value closer to 0 indicates that the model is successfully answering the questions. A value closer to 1 indicates the model is not able to answer the questions.\n\n- **Coverage** metric quantifies the extent to which the model output. It measures the percentage of output words that are also in the input text. The coverage score is a value between 0 and 1. A higher score close to 1 indicates that higher percentage of output words are within the input text.\n\n- **Density** quantifies how well the summary or the answer in the model output can be described as a series of extractions from the model input or context. A lower value of density metric indicates that on average the extractive fragments do not closely resemble verbatim extractions from the original source or context text. Abstractive summarization involves generating new, concise sentences that convey the main ideas of the source text but may not be directly copied from it. This is in contrast to extractive summarization, where the summary consists mainly of sentences directly lifted from the source. The lower the score the more abstractive the model output is.\n\n- **Abstractness** measures the abstractness of the model generated text by measuring the new n-grams generated in the model output compared to the model input. The metric computes the ratio of n-grams in the generated text that do not appear in the source content or context send to the model. The abstractness score is a value between 0 and 1. A higher score close to 1 indicates high abstractness in the generated text.\n\n- **Context Relevance** assesses the degree to which the retrieved context is relevant to the question sent to the LLM. This is one of the ways to determine the quality of your retrieval system. The context relevance score is a value between 0 and 1. A value closer to 1 indicates that the context is more relevant to your question in the prompt. A value closer to 0 indicates that the context is less relevant to your question in the prompt.\n\n- **Retrieval Precision** measures the quantity of relevant contexts from the total contexts retrieved. The retrieval precision is a value between 0 and 1. A value of 1 indicates that all the retrieved contexts are relevant. A value of 0 indicates that none of the retrieved contexts are relevant.\n\n- **Average Precision** evaluates whether all the relevant contexts are ranked higher or not. It is the mean of the precision scores of relevant contexts. The average precision is a value between 0 and 1. A value of 1 indicates that all the relevant contexts are ranked higher. A value of 0 indicates that none of the retrieved contexts are relevant.\n\n- **Reciprocal Rank** is the reciprocal of the rank of the first relevant context. The retrieval reciprocal rank is a value between 0 and 1. A value of 1 indicates that the first relevant context is at first position. A value of 0 indicates that none of the relevant contexts are retrieved.\n\n- **Hit Rate** Hit Rate measures whether there is atleast one relevant context among the retrieved contexts. The hit rate value is either 0 or 1. A value of 1 indicates that there is at least one relevant context. A value of 0 indicates that there is no relevant context in the retrieved contexts.\n\n- **Normalized Discounted Cumulative Gain** Normalized Discounted Cumulative Gain or NDCG measures the ranking quality of the retrieved contexts. The ndcg is a value between 0 and 1. A value of 1 indicates that the retrieved contexts are ranked in the correct order.", "metadata": {"id": "6515fa07-5b1a-4a53-82c5-e34266b4fc21"}}, {"cell_type": "code", "source": "import json\nfrom ibm_watsonx_ai import APIClient\n\nwml_client = APIClient(credentials)\nwml_client.version", "metadata": {"id": "8077b3d9-a583-4814-9cfb-27ac1bdbc04b"}, "outputs": [{"execution_count": 127, "output_type": "execute_result", "data": {"text/plain": "'1.3.31'"}, "metadata": {}}], "execution_count": 127}, {"cell_type": "markdown", "source": "### Function to create the access token\nThis function generates an IAM access token using the provided credentials. The API calls for creating and scoring prompt template assets utilize the token generated by this function.", "metadata": {"id": "e3bebb68-8c3c-46f3-8c30-c6926fde5d0d"}}, {"cell_type": "code", "source": "import requests, json\ndef generate_access_token():\n    headers={}\n    headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n    headers[\"Accept\"] = \"application/json\"\n    data = {\n        \"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\",\n        \"apikey\": CLOUD_API_KEY,\n        \"response_type\": \"cloud_iam\"\n    }\n    response = requests.post(IAM_URL + \"/identity/token\", data=data, headers=headers)\n    json_data = response.json()\n    iam_access_token = json_data[\"access_token\"]\n        \n    return iam_access_token\n\niam_access_token = generate_access_token()", "metadata": {"id": "1c33e5ab-54fe-40b7-b0b9-f81f17925c6c"}, "outputs": [], "execution_count": 128}, {"cell_type": "code", "source": "test_data_path = \"RAG_data.csv\"\nllm_data.to_csv(test_data_path)", "metadata": {"id": "898372f6-0411-486d-82ac-461c2a817981"}, "outputs": [], "execution_count": 129}, {"cell_type": "code", "source": "llm_data", "metadata": {"id": "1973ea78-aa07-467c-9abc-faf32ce41394"}, "outputs": [{"execution_count": 130, "output_type": "execute_result", "data": {"text/plain": "                                               query  \\\n0  What is compliance with law and firm policies ...   \n1      What are considered as Prohibited AI systems?   \n2              What is a dynamic and thriving space?   \n3                          How is work/life balance?   \n\n                                             context  \\\n0  HR_Policy_Query_Resolution_with_Retrieval_Augm...   \n1  Users (deployers) of high-risk AI systems have...   \n2  Users (deployers) of high-risk AI systems have...   \n3  HR_Policy_Query_Resolution_with_Retrieval_Augm...   \n\n                                      generated_text  \\\n0  Compliance with law and firm policies for HR p...   \n1  Prohibited AI systems, as per the AI Act, incl...   \n2  A dynamic and thriving space refers to an envi...   \n3  Work/life balance is a concept that refers to ...   \n\n                                           reference  \n0  Compliance with law and firm policies for HR p...  \n1  Prohibited AI systems, as per the AI Act, incl...  \n2  A dynamic and thriving space refers to an envi...  \n3  Work/life balance is a concept that refers to ...  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query</th>\n      <th>context</th>\n      <th>generated_text</th>\n      <th>reference</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is compliance with law and firm policies ...</td>\n      <td>HR_Policy_Query_Resolution_with_Retrieval_Augm...</td>\n      <td>Compliance with law and firm policies for HR p...</td>\n      <td>Compliance with law and firm policies for HR p...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What are considered as Prohibited AI systems?</td>\n      <td>Users (deployers) of high-risk AI systems have...</td>\n      <td>Prohibited AI systems, as per the AI Act, incl...</td>\n      <td>Prohibited AI systems, as per the AI Act, incl...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What is a dynamic and thriving space?</td>\n      <td>Users (deployers) of high-risk AI systems have...</td>\n      <td>A dynamic and thriving space refers to an envi...</td>\n      <td>A dynamic and thriving space refers to an envi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How is work/life balance?</td>\n      <td>HR_Policy_Query_Resolution_with_Retrieval_Augm...</td>\n      <td>Work/life balance is a concept that refers to ...</td>\n      <td>Work/life balance is a concept that refers to ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}], "execution_count": 130}, {"cell_type": "code", "source": "from ibm_aigov_facts_client import AIGovFactsClient\n\nfacts_client = AIGovFactsClient(\n    api_key=CLOUD_API_KEY,\n    container_id=project_id,\n    container_type=\"project\",\n    disable_tracing=True\n)", "metadata": {"id": "981e0b1d-b50c-47c9-aae9-4feaf5b99b27"}, "outputs": [], "execution_count": 131}, {"cell_type": "markdown", "source": "### Create Detached Prompt template\n\nCreate a prompt template for a retrieval augmented generation task", "metadata": {"id": "7d29366f-bc1e-42dc-bdb2-16bc158ec41d"}}, {"cell_type": "code", "source": "from ibm_aigov_facts_client import DetachedPromptTemplate, PromptTemplate\n\ndetached_information = DetachedPromptTemplate(\n    prompt_id=\"detached_prompt\",\n    model_id=\"ibm/granite-3-8b-instruct\",\n    model_provider=\"IBM\",\n    model_name=\"granite-3-8b-instruct\",\n    model_url=\"model_url\",\n    prompt_url=\"prompt_url\",\n    prompt_additional_info={\"IBM Cloud Region\": \"us-east1\"}\n)\n\ntask_id = \"retrieval_augmented_generation\"\nname = \"Agentic RAG Testing\"\ndescription = \"Agentic RAG Testing\"\nmodel_id = \"ibm/granite-3-8b-instruct\"\n\n# define parameters for PromptTemplate\nprompt_variables = {\"context\": \"\", \"query\": \"\"}\ninput = prompt_input\ninput_prefix= \"\"\noutput_prefix= \"\"\n\nprompt_template = PromptTemplate(\n    input=input,\n    prompt_variables=prompt_variables,\n    input_prefix=input_prefix,\n    output_prefix=output_prefix,\n)\n\npta_details = facts_client.assets.create_detached_prompt(\n    model_id=model_id,\n    task_id=task_id,\n    name=name,\n    description=description,\n    prompt_details=prompt_template,\n    detached_information=detached_information)\nproject_pta_id = pta_details.to_dict()[\"asset_id\"]", "metadata": {"id": "9d5eecf5-6446-4614-bf4c-1580beac1155"}, "outputs": [{"name": "stdout", "text": "2025/08/20 13:23:14 INFO : ------------------------------ Detached Prompt Creation Started ------------------------------\n2025/08/20 13:23:15 INFO : The detached prompt with ID ee5a83cc-0ec9-4f0b-98f6-efd43263ad8b was created successfully in container_id ad0092bf-2578-4e0e-9e6e-36dcf20781d0.\n", "output_type": "stream"}], "execution_count": 132}, {"cell_type": "markdown", "source": "### Configure watsonx.governance", "metadata": {"id": "5bacbe5d-fa65-4fa1-9be4-6667b8777cef"}}, {"cell_type": "code", "source": "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator, CloudPakForDataAuthenticator\n\nfrom ibm_watson_openscale import *\nfrom ibm_watson_openscale.supporting_classes.enums import *\nfrom ibm_watson_openscale.supporting_classes import *\n\nservice_instance_id = None # Update this to refer to a particular service instance\nauthenticator = IAMAuthenticator(\n    apikey=CLOUD_API_KEY,\n    url=IAM_URL\n)\nwos_client = APIClient(\n    authenticator=authenticator,\n    service_url=SERVICE_URL,\n    service_instance_id=service_instance_id\n)\ndata_mart_id = wos_client.service_instance_id\nprint(wos_client.version)", "metadata": {"id": "93b76acb-f9ea-41fb-b44c-c1dc331ca491"}, "outputs": [{"name": "stderr", "text": "WARNING:ibm_watson_openscale.client:MCSPV2Authenticator is not available. Detected Python 3.11.13. MCSP requires Python 3.12 or newer. Please install Python 3.12 and upgrade ibm-cloud-sdk-core.\n", "output_type": "stream"}, {"name": "stdout", "text": "3.0.50\n", "output_type": "stream"}], "execution_count": 133}, {"cell_type": "markdown", "source": "### For computing answer quality and retrieval quality metrics you are required to configure LLM As Judge.\n\nTo compute metrics using LLM As Judge a generative_ai_evaluator integrated system should to be created and provided during prompt setup.\n\n#### Create a Generative AI evaluator\nThe Generative AI Evaluator can be any model from watsonx.ai or a custom endpoint invoking external models\n\nSupported evaluator types\n* watsonx.ai (for connecting to watsonx.ai in local Cloud)\n* custom (for connecting to any external models)\n\n#### Integrated System parameters\n| Parameter | Description |\n|:-|:-|\n| `name` | Name for the evaluator. |\n| `description` | Description for the evaluator. |\n| `type` | The type of integrated system. Provide `generative_ai_evaluator`. |\n| `parameters` | The evaluator configuration details like `evaluator_type` and `model_id`. |\n| `credentials` | The user credentials |\n| `connection` [Optional]| The scoring endpoint details when the evaluator is of type `custom`. |\n\nAs an example, an evaluator using FLAN_UL2 model from watsonx.ai instance present in the same Cloud is created below. The other models which can be used from watsonx.ai are FLAN_T5_XXL, FLAN_UL2, FLAN_T5_XL, MIXTRAL_8X7B_INSTRUCT_V01.\nFor more details on the parameters and to create the other supported evaluators please refer to the link [Generative AI evaluator templates](https://github.ibm.com/aiopenscale/notebooks/wiki/Generative-AI-Evaluator-templates#for-ibm-watsonxgovernance-in-cloud)", "metadata": {"id": "29e746cf-2148-4c3d-a44a-e6acd09f96ce"}}, {"cell_type": "code", "source": "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n\ngen_ai_evaluator = wos_client.integrated_systems.add(\n    name=\"RAG Metrics Evaluator\",\n    description=\"RAG Metrics Evaluator\",\n    type=\"generative_ai_evaluator\",\n    parameters={\"evaluator_type\": \"watsonx.ai\", \"model_id\": \"meta-llama/llama-3-3-70b-instruct\"},\n    credentials={\n        \"wml_location\": \"cloud\",\n        \"apikey\": CLOUD_API_KEY,\n    },\n)\n\n# get evaluator integrated system ID\nresult = gen_ai_evaluator.result._to_dict()\nevaluator_id = result[\"metadata\"][\"id\"]\nevaluator_id", "metadata": {"id": "275e6840-0aa2-461f-af1e-da371297b57c"}, "outputs": [{"execution_count": 144, "output_type": "execute_result", "data": {"text/plain": "'0198c7b5-d696-7458-bef7-50316d53065f'"}, "metadata": {}}], "execution_count": 144}, {"cell_type": "markdown", "source": "#### Generative AI monitor parameters\n\n##### Generative AI evaluator parameters\nThe generative_ai_evaluator details can be provided at the global level under `generative_ai_quality.parameters` to use the same evaluator for all the Answer quality metrics(Faithfulness, Answer relevance, Answer similarity) and Retrieval quality metrics(Context relevance, Retrieval precision, Average precision, Reciprocal rank, Hit rate, Normalized Discounted Cumulative Gain).\n\nThe generative ai evaluator can be specified at metric level to use different evaluators for each of the metric. The metrics under which generative_ai_evaluator parameter can be specified are faithfulness, answer_relevance, answer_similarity and retrieval_quality. The generative_ai_evaluator at the metric level takes precedence over the generative_ai_evaluator at global level\n\n| Parameter | Description | Default Value |\n|:-|:-|:-|\n| `enabled`| The flag to enable generative ai evaluator |  |\n| `evaluator_id`| The id of the generative ai evaluator integrated system. |  |\n\n##### Faithfulness, Context relevance, Answer relevance and Answer similarity parameters\n\n| Parameter | Description | Default Value |\n|:-|:-|:-|\n| `metric_prompt_template` [Optional]| The prompt template used to compute each metric values. User can override the prompt template used by watsonx.governance to compute the metric using this parameter. The prompt template should use the variables {context}, {question}, {answer}, {reference_answer} as needed and these variable values will be filled with the actual data while calling the scoring function. The prompt response should return the metric value in the range 1-10 for the respective metric and in one of the formats [\"4\", \"7 star\", \"star: 8\", \"stars: 9\"] as answer. |  |\n\n##### Unsuccessful requests parameters\n\n| Parameter | Description | Default Value |\n|:-|:-|:-|\n| `unsuccessful_phrases` [Optional]| The list of phrases to be used for comparing the model output to determine whether the request is unsuccessful or not. | `[\"i don't know\", \"i do not know\", \"i'm not sure\", \"i am not sure\", \"i'm unsure\", \"i am unsure\", \"i'm uncertain\", \"i am uncertain\", \"i'm not certain\", \"i am not certain\", \"i can't fulfill\", \"i cannot fulfill\"]` |\n", "metadata": {"id": "6502e690-a0eb-49a4-b23e-f5772d68f70b"}}, {"cell_type": "markdown", "source": "## Configure Answer Quality metrics & Retrieval Quality metrics\n<a id=\"config\"></a>", "metadata": {"id": "8a628b3f-21cd-4ff0-9271-97db5e84bffd"}}, {"cell_type": "markdown", "source": "### Parameters", "metadata": {"id": "93eeb5fd-096a-41a8-a7f5-24eb65f40245"}}, {"cell_type": "markdown", "source": "#### Common parameters\n\n| Parameter | Description | Default Value | Possible Value(s) |\n|:-|:-|:-|:-|\n| context_columns | The list of context column names in the input data frame. |  |  |\n| question_column | the name of the question column in the input data frame. |  |  |\n| answer_column | The name of the answer column in the input data frame |  |  |\n| record_level | The flag to return the record level metrics values. Set the flag under configuration to generate record level metrics for all the metrics. Set the flag under specific metric to generate record level metrics for that metric alone. | `False` | `True`, `False` |", "metadata": {"id": "41c693ff-2f92-47cd-882a-06cc83b3e2fb"}}, {"cell_type": "markdown", "source": "#### Faithfulness parameters\n| Parameter | Description | Default Value | Possible Value(s) |\n|:-|:-|:-|:-|\n| attributions_count [Optional]| Source attributions are computed for each sentence in the generated answer. Source attribution for a sentence is the set of sentences in the context which contributed to the LLM generating that sentence in the answer.  The attributions_count parameter specifies the number of sentences in the context which need to be identified for attributions.  E.g., if the value is set to 2, then we will find the top 2 sentences from the context as source attributions. | `3` |  |\n| ngrams [Optional]| The number of sentences to be grouped from the context when computing faithfulness score. These grouped sentences will be shown in the attributions. Having a very high value of ngrams might lead to having lower faithfulness scores due to dispersion of data and inclusion of unrelated sentences in the attributions. Having a very low value might lead to increase in metric computation time and attributions not capturing the all the aspects of the answer. | `2` |  |\n| sample_size [Optional]| The faithfulness metric is computed for a maximum of 50 LLM responses.  If you wish to compute it for a smaller number of responses, set the sample_size value to a lower number. If the number of records in the input data frame are more than the sample size, a uniform random sample will taken for computation. | `50` | Integer between 0 to 50. Max value supported is 50 |\n| record_level [Optional]| Set the flag to generate record level metrics for the specific metric. | `False` | `True`, `False` |\n", "metadata": {"id": "7159495a-6ae2-4569-b588-22f74398081a"}}, {"cell_type": "markdown", "source": "#### Context relevance parameters\n| Parameter | Description | Default Value | Possible Value(s) |\n|:-|:-|:-|:-|\n| ngrams [Optional]| The number of sentences to be grouped from the context when computing context relevance score. Having a very high value of ngrams might lead to having lower context relevance scores due to dispersion of data and inclusion of unrelated sentences. | `5` |  |", "metadata": {"id": "514eb681-aee3-42db-b9d5-43dfb0c012e0"}}, {"cell_type": "markdown", "source": "\n#### Unsuccessful requests parameters\n| Parameter | Description | Default Value |\n|:-|:-|:-|\n| unsuccessful_phrases [Optional]| The list of phrases to be used for comparing the model output to determine whether the request is unsuccessful or not. | `[\"i don't know\", \"i do not know\", \"i'm not sure\", \"i am not sure\", \"i'm unsure\", \"i am unsure\", \"i'm uncertain\", \"i am uncertain\", \"i'm not certain\", \"i am not certain\", \"i can't fulfill\", \"i cannot fulfill\"]` |", "metadata": {"id": "e9235345-55f2-4fee-81c7-0cf8620507b4"}}, {"cell_type": "markdown", "source": "### Configure faithfulness, answer relevance, and unsuccessful requests parameters", "metadata": {"id": "c7b6546b-f3cf-4264-a249-0a0bb273970f"}}, {"cell_type": "code", "source": "label_column = \"reference\"\ncontext_fields = [\"context\"]\nquestion_field = \"query\"\noperational_space_id = \"development\"\nproblem_type= \"retrieval_augmented_generation\"\ninput_data_type= \"unstructured_text\"\n\nmonitors = {\n    \"generative_ai_quality\": {\n        \"parameters\": {\n            \"generative_ai_evaluator\": { # global LLM as judge configuration\n               \"enabled\": True,\n               \"evaluator_id\": evaluator_id,\n            },\n            \"min_sample_size\": 2,\n            \"metrics_configuration\": {\n                \"faithfulness\": {\n                    # \"metric_prompt_template\": \"\", # adding custom template\n                    # Uncomment generative_ai_evaluator to use a different evaluator for this metric.\n                    # Takes higher precedence than the generative_ai_evaluator specified at global level.\n                    # \"generative_ai_evaluator\": {  # metric specific LLM as judge configuration\n                    #     \"enabled\": True,\n                    #     \"evaluator_id\": evaluator_id,\n                    # },\n                },\n                \"answer_relevance\": {\n                    # \"metric_prompt_template\": \"\", # adding custom template\n                    # Uncomment generative_ai_evaluator to use a different evaluator for this metric\n                    # Takes higher precedence than the generative_ai_evaluator specified at global level.\n                    # \"generative_ai_evaluator\": {  # metric specific LLM as judge configuration\n                    #     \"enabled\": True,\n                    #     \"evaluator_id\": evaluator_id,\n                    # },\n                },\n                \"rouge_score\": {},\n                \"exact_match\": {},\n                \"bleu\": {},\n                \"unsuccessful_requests\": {\n                    # \"unsuccessful_phrases\": []\n                },\n                \"hap_input_score\": {},\n                \"hap_score\": {},\n                \"pii\": {},\n                \"pii_input\": {},\n                \"retrieval_quality\": {\n                    # Uncomment generative_ai_evaluator to use a different evaluator for this metric\n                    # Takes higher precedence than the generative_ai_evaluator specified at global level.\n                    # \"generative_ai_evaluator\": {  # metric specific LLM as judge configuration\n                    #     \"enabled\": True,\n                    #     \"evaluator_id\": evaluator_id,\n                    # },\n                    # The metrics computed for retrieval quality are context_relevance, retrieval_precision, average_precision, reciprocal_rank, hit_rate, normalized_discounted_cumulative_gain\n                    # \"context_relevance\": {\n                    #     \"metric_prompt_template\": \"\", # adding custom template\n                    # }\n                },\n                # Answer similarity metric is supported only when LLM as judge is configured. Uncomment only when using LLM as judge.\n                \"answer_similarity\": {\n                    # \"metric_prompt_template\": \"\", # adding custom template\n                    # Uncomment generative_ai_evaluator to use a different evaluator for this metric\n                    # Takes higher precedence than the generative_ai_evaluator specified at global level.\n                    # \"generative_ai_evaluator\": {  # metric specific LLM as judge configuration\n                    #     \"enabled\": True,\n                    #     \"evaluator_id\": evaluator_id,\n                    # },\n                },\n            },\n        }\n    }\n}\n\nresponse = wos_client.monitor_instances.mrm.execute_prompt_setup(\n    prompt_template_asset_id=project_pta_id, \n    project_id=project_id,\n    label_column=label_column,\n    context_fields = context_fields,     \n    question_field = question_field,     \n    operational_space_id=operational_space_id, \n    problem_type=problem_type,\n    input_data_type=input_data_type, \n    supporting_monitors=monitors, \n    background_mode=False\n)\n\nresult = response.result\nresult.to_dict()", "metadata": {"id": "58edc161-6e5e-4034-b992-a2ee4b86082f"}, "outputs": [{"name": "stdout", "text": "=================================================================\nThis method will be deprecated in the next release and be replaced by wos_client.wos.execute_prompt_setup() method\n=================================================================\n\n\n=============================================================================\n\n Waiting for end of adding prompt setup ee5a83cc-0ec9-4f0b-98f6-efd43263ad8b \n\n=============================================================================\n\n\n\nfinished\n\n---------------------------------------------------------------\n Successfully finished setting up prompt template subscription \n---------------------------------------------------------------\n\n\n", "output_type": "stream"}, {"execution_count": 145, "output_type": "execute_result", "data": {"text/plain": "{'prompt_template_asset_id': 'ee5a83cc-0ec9-4f0b-98f6-efd43263ad8b',\n 'project_id': 'ad0092bf-2578-4e0e-9e6e-36dcf20781d0',\n 'deployment_id': 'c1fb152c-c7d2-4b8c-85e3-c0326f471ad8',\n 'service_provider_id': '0198c1d2-6e80-7e5e-b63a-b5b3f7f90398',\n 'subscription_id': '0198c7a6-0c9d-709a-b035-0c9a345d8c38',\n 'mrm_monitor_instance_id': '0198c7a6-3559-72c5-9405-fd9266ff5b33',\n 'start_time': '2025-08-20T13:40:53.834854Z',\n 'end_time': '2025-08-20T13:40:59.316416Z',\n 'status': {'state': 'FINISHED'}}"}, "metadata": {}}], "execution_count": 145}, {"cell_type": "markdown", "source": "With the following cell, you can read the prompt setup task and check its status", "metadata": {"id": "c129e9d5-52b9-4c78-8385-ace144617eb9"}}, {"cell_type": "code", "source": "response = wos_client.monitor_instances.mrm.get_prompt_setup(\n    prompt_template_asset_id=project_pta_id,\n    project_id=project_id\n)\n\nresult = response.result\nresult_json = result.to_dict()\n\nif result_json[\"status\"][\"state\"] == \"FINISHED\":\n    print(\"Finished prompt setup. The response is {}\".format(result_json))\nelse:\n    print(\"Prompt setup failed. The response is {}\".format(result_json))", "metadata": {"id": "4d903cbc-64d3-4be8-bd6d-75ed808aa276"}, "outputs": [{"name": "stdout", "text": "=================================================================\nThis method will be deprecated in the next release and be replaced by wos_client.wos.get_prompt_setup() method\n=================================================================\nFinished prompt setup. The response is {'prompt_template_asset_id': 'ee5a83cc-0ec9-4f0b-98f6-efd43263ad8b', 'project_id': 'ad0092bf-2578-4e0e-9e6e-36dcf20781d0', 'deployment_id': 'c1fb152c-c7d2-4b8c-85e3-c0326f471ad8', 'service_provider_id': '0198c1d2-6e80-7e5e-b63a-b5b3f7f90398', 'subscription_id': '0198c7a6-0c9d-709a-b035-0c9a345d8c38', 'mrm_monitor_instance_id': '0198c7a6-3559-72c5-9405-fd9266ff5b33', 'start_time': '2025-08-20T13:40:53.834854Z', 'end_time': '2025-08-20T13:40:59.316416Z', 'status': {'state': 'FINISHED'}}\n", "output_type": "stream"}], "execution_count": 146}, {"cell_type": "code", "source": "subscription_id = result_json[\"subscription_id\"]\nmrm_monitor_instance_id = result_json[\"mrm_monitor_instance_id\"]", "metadata": {"id": "b2d49409-24cf-42c1-9982-f3c474b829a3"}, "outputs": [], "execution_count": 147}, {"cell_type": "markdown", "source": "### Show all monitor instances in the development subscription\nThe following cell lists the monitors present in the development subscription, along with their respective statuses and other details. Please wait for all the monitors to be in an active state before proceeding further.", "metadata": {"id": "56709bff-f7d6-4d57-aac6-836bedb32599"}}, {"cell_type": "code", "source": "wos_client.monitor_instances.show(target_target_id=subscription_id)", "metadata": {"id": "ab997dfb-c8a9-45b9-b977-ec7fc860702f"}, "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<HTML>\n        <body>\n            <h3>Monitor instances</h3>\n            <table style='border: 1px solid #dddddd; font-family: Courier'>\n                <th style='border: 1px solid #dddddd'>data_mart_id</th><th style='border: 1px solid #dddddd'>status</th><th style='border: 1px solid #dddddd'>target_id</th><th style='border: 1px solid #dddddd'>target_type</th><th style='border: 1px solid #dddddd'>monitor_definition_id</th><th style='border: 1px solid #dddddd'>created_at</th><th style='border: 1px solid #dddddd'>id</th>\n                <tr><td style='border: 1px solid #dddddd'>67f4a470-900f-4a28-beee-af3f9b02ed11</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>0198c7a6-0c9d-709a-b035-0c9a345d8c38</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>mrm</td><td style='border: 1px solid #dddddd'>2025-08-20 13:23:36.252000+00:00</td><td style='border: 1px solid #dddddd'>0198c7a6-3559-72c5-9405-fd9266ff5b33</td></tr><tr><td style='border: 1px solid #dddddd'>67f4a470-900f-4a28-beee-af3f9b02ed11</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>0198c7a6-0c9d-709a-b035-0c9a345d8c38</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>model_health</td><td style='border: 1px solid #dddddd'>2025-08-20 13:23:34.467000+00:00</td><td style='border: 1px solid #dddddd'>0198c7a6-2c0d-70bb-96b2-3ebb91631f80</td></tr><tr><td style='border: 1px solid #dddddd'>67f4a470-900f-4a28-beee-af3f9b02ed11</td><td style='border: 1px solid #dddddd'>active</td><td style='border: 1px solid #dddddd'>0198c7a6-0c9d-709a-b035-0c9a345d8c38</td><td style='border: 1px solid #dddddd'>subscription</td><td style='border: 1px solid #dddddd'>generative_ai_quality</td><td style='border: 1px solid #dddddd'>2025-08-20 13:23:32.362000+00:00</td><td style='border: 1px solid #dddddd'>0198c7a6-2624-7969-94d3-e40010ded1df</td></tr>\n            </table>\n        </body>\n        </HTML>"}, "metadata": {}}], "execution_count": 148}, {"cell_type": "markdown", "source": "### Risk evaluations for the PTA subscription - Evaluate the prompt template subscription\n\nFor risk assessment of a `development`-type subscription, you must have an evaluation dataset. The risk assessment function takes the evaluation dataset path as a parameter when evaluating the configured metrics. If there is a discrepancy between the feature columns in the subscription and the column names in the uploading `.CSV` file, you have the option to supply a mapping JSON file to associate the `.CSV` column names with the feature column names in the subscription.\n\n**Note**: If you are running this notebook from Watson Studio, you may first need to upload your test data to Watson Studio, then run the code snippet below to download the feedback data file from the project to a local directory.", "metadata": {"id": "0c8768ff-b8eb-4d33-9996-941d70343a47"}}, {"cell_type": "code", "source": "test_data_set_name = \"data\"\ncontent_type = \"multipart/form-data\"\nbody = {}\n\n# Preparing the test data, removing extra columns\ncols_to_remove = [\"uid\", \"doc\", \"title\", \"id\"]\nfor col in cols_to_remove:\n    if col in llm_data:\n        del llm_data[col]\nllm_data.to_csv(test_data_path, index=False)\n\nresponse  = wos_client.monitor_instances.mrm.evaluate_risk(\n    monitor_instance_id=mrm_monitor_instance_id,\n    test_data_set_name=test_data_set_name, \n    test_data_path=test_data_path,\n    content_type=content_type,\n    body=body,\n    project_id=project_id,\n    includes_model_output=True,\n    background_mode=False\n)", "metadata": {"id": "51072101-0163-4d50-9f99-de781a64cad0", "msg_id": "e82795aa-7322-4ea6-a912-9cb632faf4d4"}, "outputs": [{"name": "stdout", "text": "\n\n=================================================================================\n\n Waiting for risk evaluation of MRM monitor 0198c7a6-3559-72c5-9405-fd9266ff5b33 \n\n=================================================================================\n\n\n\nupload_in_progress", "output_type": "stream"}], "execution_count": null}, {"cell_type": "code", "source": "response  = wos_client.monitor_instances.mrm.get_risk_evaluation(mrm_monitor_instance_id, project_id=project_id)\nresponse.result.to_dict()", "metadata": {"id": "6aa46ced-2a61-4c65-8a1e-99a805d3093f", "msg_id": "440adfbd-8ad7-4199-93d9-c03001b51272"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Display the Model Risk metrics.\n\nHaving calculated the measurements for the Foundation Model subscription, the Model Risk metrics generated for this subscription are available for your review:", "metadata": {"id": "29ac7313-a9c6-4699-8a2a-07bfece896da"}}, {"cell_type": "code", "source": "wos_client.monitor_instances.show_metrics(monitor_instance_id=mrm_monitor_instance_id, project_id=project_id)", "metadata": {"id": "082b49fa-6664-4d3d-99cc-0208a9e01d3c", "msg_id": "40f46237-79a1-417f-b2c5-192db93cf083"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "monitor_definition_id = \"generative_ai_quality\"\nresult = wos_client.monitor_instances.list(\n    data_mart_id=data_mart_id,\n    monitor_definition_id=monitor_definition_id,\n    target_target_id=subscription_id,\n    project_id=project_id\n).result\nresult_json = result._to_dict()\ngenaiquality_monitor_id = result_json[\"monitor_instances\"][0][\"metadata\"][\"id\"]\ngenaiquality_monitor_id", "metadata": {"id": "42aea40b-2981-4a53-8610-421a5a98cc1e", "msg_id": "78db2b17-2ecd-4433-bf5b-577b821bf007"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Display the Generative AI quality metrics", "metadata": {"id": "2d402b72-4a53-4d17-a94c-d590a45ecc50"}}, {"cell_type": "code", "source": "wos_client.monitor_instances.show_metrics(monitor_instance_id=genaiquality_monitor_id, project_id=project_id)", "metadata": {"id": "36e57b27-4387-4313-baee-40ebcb90bb1d", "msg_id": "cedb53c0-ef5a-492f-b47a-8cfa814325a2"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Congratulations!\n\nYou have completed this notebook. You can now navigate to the prompt template asset in your watsonx.governance project / space and click on the `Evaluate` tab to visualize the results in the UI.", "metadata": {"id": "32dc6e4e-af89-4130-a24e-61b450769379"}}, {"cell_type": "code", "source": "", "metadata": {"id": "48f3d37b-0fe7-489c-918e-af363d7c9ccc"}, "outputs": [], "execution_count": null}]}
