{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# Milvus collection build for the grounded collection\nThis notebook contains steps and code to populate a Milvus collection using the documents defined in\nthe associated vector index asset. If the collection is not empty, it will be emptied before being re-populated.\n\n**Note:** Notebook code generated using the Vector Index tool will execute successfully.\nIf code is modified or reordered, there is no guarantee it will successfully execute.\n\nSome familiarity with Python is helpful. This notebook uses Python 3.11.\n\n## Contents\nThis notebook contains the following parts:\n\n1. Setup\n2. Create vector store\n3. Process input documents\n4. Load chunked documents into the vector store"}, {"metadata": {}, "cell_type": "markdown", "source": "## Setup\nInstall the libraries you need for this notebook."}, {"metadata": {}, "cell_type": "code", "source": "!pip install 'ibm-watsonx-ai>=1.3.6,<1.4.0'\n!pip install python-pptx\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport getpass\n\nimport ibm_boto3\nfrom ibm_botocore.client import Config\nfrom ibm_watsonx_ai.client import APIClient\nfrom ibm_watsonx_ai.foundation_models.extensions.rag.vector_stores import VectorStore\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_core.documents import Document\nfrom ibm_watsonx_ai.foundation_models.extensions.rag.chunker import LangChainChunker\nfrom langchain.document_loaders import (\n  CSVLoader,\n  UnstructuredExcelLoader\n)\n\nfrom ibm_watsonx_ai.data_loaders.datasets.documents import DocumentsIterableDataset\nfrom ibm_watsonx_ai.data_loaders.experiment import ExperimentDataLoader\nfrom ibm_watsonx_ai.helpers import FSLocation, AssetLocation\nfrom ibm_watsonx_ai.helpers.connections import DataConnection\n\nfrom ibm_watsonx_ai.foundation_models import Embeddings\nfrom ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Connection to WML\nThis cell defines the credentials required to work with watsonx API for both the execution of the build.\n\n**Action:** Provide the IBM Cloud personal API key. For details, see\n<a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\">documentation</a>.\n"}, {"metadata": {}, "cell_type": "code", "source": "# NOTE: this cell uses a VECTOR_INDEX_BEARER_TOKEN variable for job runs. We need to make the modification\n# to use customer's API key as defined in user profile. Otherwise asking for user input\n\nwml_credentials = {\n    \"url\": \"https://ca-tor.ml.cloud.ibm.com\",\n}\nif (os.getenv(\"VECTOR_INDEX_BEARER_TOKEN\")):\n    wml_credentials[\"token\"] = os.getenv(\"VECTOR_INDEX_BEARER_TOKEN\")\nelse:\n    wml_credentials[\"apikey\"] = getpass.getpass(\"Enter your API key:\")\n\nproject_id = os.getenv(\"PROJECT_ID\")\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "client = APIClient(wml_credentials=wml_credentials)\nclient.set.default_project(project_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Create vector store\nCreate an instance of the vector store wrapper class."}, {"metadata": {}, "cell_type": "code", "source": "vector_index_details = client.data_assets.get_details(\"c421728b-6e37-43aa-b5d8-612e74154323\")\nvector_index_properties = vector_index_details[\"entity\"][\"vector_index\"]\nprint(vector_index_properties)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "emb = Embeddings(\n    model_id=vector_index_properties[\"settings\"][\"embedding_model_id\"],\n    credentials=wml_credentials,\n    project_id=project_id,\n    params={\n        \"truncate_input_tokens\": 512\n    }\n)\n\nindex_name = vector_index_properties[\"store\"][\"index\"]\ndatabase_name = vector_index_properties[\"store\"][\"database\"]\n\ntext_field = None\n\nif (\"schema_fields\" in vector_index_properties[\"settings\"]):\n    vector_store_schema = vector_index_properties[\"settings\"][\"schema_fields\"]\n    text_field = vector_store_schema.get(\"text\")\n\nvector_store = VectorStore(\n    client=client,\n    connection_id=vector_index_properties[\"store\"][\"connection_id\"],\n    embeddings=emb,\n    index_name=index_name,\n    drop_old=True,\n    database=database_name,\n    consistency_level='Strong',\n    connection_args={'secure': True},\n    text_field=text_field\n)\n\n\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Process the documents\nWe will now loop through the list of documents in the vector index, process them one by one according to their mime type."}, {"metadata": {}, "cell_type": "code", "source": "mime_type_mappings = {\n    'text/csv': CSVLoader\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "chunk_size = vector_index_properties[\"settings\"][\"chunk_size\"]\nchunk_overlap = vector_index_properties[\"settings\"][\"chunk_overlap\"]\n\ntext_splitter = LangChainChunker(\n    method=\"recursive\",\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def load_document( document_properties ):\n    id = document_properties[\"metadata\"][\"asset_id\"]\n    filename = document_properties[\"metadata\"][\"name\"]\n    file_path = client.data_assets.download(id, filename)\n    mime_type = document_properties[\"entity\"][\"data_asset\"][\"mime_type\"]\n    if (mime_type == \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"):\n        dataset = DocumentsIterableDataset(connections=[DataConnection(location=FSLocation(path=file_path))], error_callback=error_callback)\n        data_loader = ExperimentDataLoader(dataset=dataset)\n        return data_loader\n    if (mime_type_mappings.get(mime_type)):\n        loader = mime_type_mappings[mime_type](file_path)\n        return loader.load_and_split()\n    dataset = DocumentsIterableDataset(connections=[DataConnection(location=FSLocation(path=file_path))], error_callback=error_callback)\n    data_loader = ExperimentDataLoader(dataset=dataset)\n    return data_loader\n\ndef error_callback(document_name: str, exception: Exception):\n   raise Exception(f\" loading document: {document_name} failed with Exception: \\n{exception}\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "### The vector store schema is stored using dot notation\n### This converts the dot notation to an object consumable by the Document\ndef dot_notation_to_dict(dot_notation, value, metadata):\n    if dot_notation == None:\n        return metadata\n    parts = dot_notation.split('.')\n    current = metadata\n    for part in parts[:-1]:\n        if part not in current:\n            current[part] = {}\n        current = current[part]\n    current[parts[-1]] = value\n    return metadata\n\ndef compute_documents_metadata( document_name, loaded_documents ):\n    filtered_documents = []\n    if (\"schema_fields\" in vector_index_properties[\"settings\"]):\n        vector_store_schema = vector_index_properties[\"settings\"][\"schema_fields\"]\n        for document in loaded_documents:\n            computed_document_data = {\n                \"metadata\": {}\n            }\n            computed_document_data[\"metadata\"] = dot_notation_to_dict(vector_store_schema.get(\"page_number\"), document.dict()[\"metadata\"].get(\"page\", 0), computed_document_data[\"metadata\"])\n            computed_document_data[\"metadata\"] = dot_notation_to_dict(vector_store_schema.get(\"document_name\"), document_name, computed_document_data[\"metadata\"])\n            computed_document_data[\"metadata\"] = dot_notation_to_dict(vector_store_schema.get(\"page_content\"), document.dict()[\"page_content\"], computed_document_data[\"metadata\"])\n            filtered_documents.append(document.copy(update=computed_document_data))\n    return filtered_documents\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def process_document( document_properties ):\n    print(\"Processing\", document_properties[\"metadata\"][\"name\"])\n    # Parse the document into raw text\n    loaded_documents = load_document(document_properties)\n    filtered_documents = compute_documents_metadata(document_properties[\"metadata\"][\"name\"], loaded_documents)\n    # Split the document text into chunks\n    return text_splitter.split_documents(filtered_documents)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def process_documents_from_folder( connected_folder_details ):\n    connection_id = connected_folder_details[\"entity\"][\"folder_asset\"][\"connection_id\"]\n    connection_path = connected_folder_details[\"entity\"][\"folder_asset\"][\"connection_path\"]\n    connection_details = client.connections.get_details(connection_id)\n    connection_props = connection_details['entity']['properties']\n    connection_name = connection_details['entity']['name']\n    bucket_name=connection_props.get(\"bucket\")\n\n    ## Do not process documents if there is no bucket defined in the connection\n    if bucket_name == None:\n        return []\n    \n    print(f\"Initializing client for {connection_name} connection\")\n\n    cos_client = ibm_boto3.client(\n        \"s3\",\n        ibm_api_key_id=connection_props['api_key'],\n        ibm_service_instance_id=connection_props['resource_instance_id'],\n        ibm_auth_endpoint=connection_props['iam_url'],\n        config=Config(signature_version=\"oauth\"),\n        endpoint_url=f\"https://{connection_props['url']}\"\n    )\n    \n    prefix=connection_path.removeprefix(f\"/{bucket_name}/\")\n    prefix=f\"{prefix}/\"\n\n    files = cos_client.list_objects(Bucket=bucket_name, Prefix=prefix)\n    filenames = [f.get('Key') for f in files.get(\"Contents\", [])]\n    all_documents = []\n    for filename in filenames:\n        loaded_documents = []\n        try:\n            file_path = f\"{connection_name}{filename.removeprefix(prefix)}\"\n            cos_client.download_file(Bucket=bucket_name, Filename=file_path, Key=filename)\n            file_props = cos_client.get_object(Bucket=bucket_name, Key=filename)\n            mime_type = file_props[\"ContentType\"]\n\n            # Parse the document into raw text\n            if (mime_type == \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"):\n                dataset = DocumentsIterableDataset(connections=[DataConnection(location=FSLocation(path=file_path))], error_callback=error_callback)\n                data_loader = ExperimentDataLoader(dataset=dataset)\n                loaded_documents += data_loader\n            elif (mime_type_mappings.get(mime_type)):\n                loader = mime_type_mappings[mime_type](file_path)\n                loaded_documents += loader.load_and_split()\n            else:\n                dataset = DocumentsIterableDataset(connections=[DataConnection(location=FSLocation(path=file_path))], error_callback=error_callback)\n                data_loader = ExperimentDataLoader(dataset=dataset)\n                loaded_documents += data_loader\n            \n            all_documents += compute_documents_metadata(f\"{connection_name}-{filename.removeprefix(prefix)}\", loaded_documents)\n            \n            print(f\"Loaded '{filename}' from '{bucket_name}' bucket.\")\n        except Exception as e:\n            print(f\"Could not load '{filename}' from '{bucket_name}' bucket.\", e)\n\n    # Split the document text into chunks\n    return text_splitter.split_documents(all_documents)\n\ndef error_callback(document_name: str, exception: Exception):\n   raise Exception(f\" loading document: {document_name} failed with Exception: \\n{exception}\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def process_documents( document_ids ):\n    documents = []\n    for document_id in document_ids:\n        document_properties = client.data_assets.get_details(document_id)\n        \n        if (document_properties[\"metadata\"][\"asset_type\"] == \"folder_asset\"):\n            document = process_documents_from_folder(document_properties)\n            documents += document\n        else:\n            document = process_document(document_properties)\n            documents += document\n    \n    return documents", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "documents = process_documents(vector_index_properties[\"data_assets\"])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Load chunked documents into the vector store\nWe can now add the chunked documents into the vector store. As the build notebook can be\nrun over and over, we need to ensure we are working with an empty index."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "vector_store.add_documents(content=documents, batch_size=20)\nprint(\"Documents were added.\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Next steps\nYou successfully executed this notebook! If there were no errors, the vector index has been loaded\nwith document chunks and is now ready for executing proximity search queries.\n\n<a id=\"copyrights\"></a>\n### Copyrights\n\nLicensed Materials - Copyright \u00a9 2024 IBM. This notebook and its source code are released under the terms of the ILAN License.\nUse, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n\n**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for watsonx.ai Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for watsonx.ai Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n\nBy downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\" target=\"_blank\">License Terms</a>  "}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}}, "nbformat": 4, "nbformat_minor": 0}