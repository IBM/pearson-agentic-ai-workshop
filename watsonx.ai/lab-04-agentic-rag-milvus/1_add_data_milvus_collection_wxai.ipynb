{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52336261-d661-41a1-b4aa-6f18036f3840"
   },
   "outputs": [],
   "source": [
    "!pip install langchain_core==0.3.75 langchain_huggingface==0.3.1 langchain-community sentence-transformers\n",
    "!pip install torch torchvision --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dace7d2f-9d40-40c4-880d-e54799c4a853"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# import toml\n",
    "\n",
    "from pymilvus import connections\n",
    "from pymilvus import Collection, utility, connections, FieldSchema, CollectionSchema, DataType\n",
    "\n",
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config, ClientError\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Updated import\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1e8463d-6e27-4792-8b56-93aac594d21e"
   },
   "source": [
    "## Load connections information from the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0b266b4-927e-4b5c-bb28-bd3956414826"
   },
   "outputs": [],
   "source": [
    "# list your connections\n",
    "wslib.list_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7f238bd-816f-4048-96de-4709c2ac9a01"
   },
   "outputs": [],
   "source": [
    "# make sure you use the right connection name for presto\n",
    "milvus_conn = wslib.get_connection('milvus_connection')\n",
    "cos_conn = wslib.get_connection('cos_connection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d8a1c4f-4c75-424a-b3b0-432f9e6be5fa"
   },
   "source": [
    "## Load env.txt file with configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d978cf61-ba78-4891-9590-6f8d299966b5"
   },
   "outputs": [],
   "source": [
    "with open('.env_all', 'wb') as env_file:\n",
    "    env_file.write(wslib.load_data('env.txt').read())\n",
    "# environmental variables store credentials and configuration\n",
    "load_dotenv('.env_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67f8002e-06e3-42da-b5fd-d2835d6e7322"
   },
   "source": [
    "## COS Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for IBM COS values\n",
    "COS_ENDPOINT = f\"https://{cos_conn['url']}\"\n",
    "COS_API_KEY_ID = cos_conn['api_key']\n",
    "COS_INSTANCE_CRN = cos_conn['resource_instance_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make sure that COS_ENDPOINT is the same as for buckets and the same for all buckets, if not -> replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(COS_ENDPOINT)\n",
    "# COS_ENDPOINT = \"enter https-prepended endpoint and uncomment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75c174e3-22df-492f-9814-22bd4cd76dac"
   },
   "outputs": [],
   "source": [
    "# Create client\n",
    "cos_resource = ibm_boto3.resource(\"s3\",\n",
    "    ibm_api_key_id=COS_API_KEY_ID,\n",
    "    ibm_service_instance_id=COS_INSTANCE_CRN,\n",
    "    config=Config(signature_version=\"oauth\"),\n",
    "    endpoint_url=COS_ENDPOINT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For text splitting and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15635fef-5a73-4856-ba44-a1175e6decb5"
   },
   "outputs": [],
   "source": [
    "embeddings_model_name = os.getenv(\"SENTENCE_TRANSFORMER\")\n",
    "chunk_size = int(os.getenv(\"TEXT_SPLITTER_CHUNK_SIZE\"))\n",
    "chunk_overlap = int(os.getenv(\"TEXT_SPLITTER_CHUNK_OVERLAP\"))\n",
    "\n",
    "# TEXT - SPLITTER RELATED\n",
    "text_splitter_type = os.getenv(\"TEXT_SPLITTER_TYPE\")\n",
    "# to define text splitter incl. text splitter separators (defined in the environment variables)\n",
    "# json string\n",
    "text_splitter_separators = os.getenv(\"TEXT_SPLITTER_SEPARATORS\")\n",
    "text_replacements = os.getenv(\"TEXT_REPLACEMENTS\")\n",
    "\n",
    "similarity_metric = os.getenv(\"SIMILARITY_METRIC\")\n",
    "\n",
    "INPUT_BUCKET = os.environ[\"INPUT_BUCKET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "625a6d22-f0b3-4bc6-8d4a-f1978f3bbfdb"
   },
   "outputs": [],
   "source": [
    "# embeddings function - TO SELECT FUNCTION and PARAMETERS\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For milvus connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8db4731-194c-4420-aa4a-ccd374a32c4b"
   },
   "outputs": [],
   "source": [
    "# Authentication for Milvus within wx.data\n",
    "host = milvus_conn[\"host\"]\n",
    "port = milvus_conn[\"port\"]\n",
    "password = milvus_conn[\"password\"]\n",
    "user = milvus_conn[\"username\"]\n",
    "\n",
    "mv_collection = os.environ[\"MV_COLLECTION_NAME\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "358e9d07-9108-41de-aca0-f98a1a422e8f"
   },
   "outputs": [],
   "source": [
    "connection_args={\n",
    "               'host':host,\n",
    "               'port':port,\n",
    "               'user': user,\n",
    "               'password': password,\n",
    "                'secure': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96147576-087e-41b2-8789-9614a7539f9f"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import posixpath\n",
    "\n",
    "from ibm_botocore.client import ClientError\n",
    "\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    ")\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "# dictionary with text splitter classes\n",
    "text_splitter_type_dict = {\n",
    "    \"CharacterTextSplitter\": CharacterTextSplitter,\n",
    "    \"RecursiveCharacterTextSplitter\": RecursiveCharacterTextSplitter,\n",
    "}\n",
    "\n",
    "# dictionary with available document loaders\n",
    "_document_loaders_dict = {\n",
    "    \"docx\": Docx2txtLoader,\n",
    "    \"html\": UnstructuredHTMLLoader,\n",
    "    \"pdf\": PyPDFLoader,\n",
    "    \"csv\": TextLoader,\n",
    "    \"txt\": TextLoader,\n",
    "}\n",
    "\n",
    "def get_bucket_contents(cur_conn, bucket_name):\n",
    "    \"\"\"\n",
    "    function to get the list of files from the given bucket\n",
    "    \"\"\"\n",
    "    items_list = []\n",
    "    print(\"Retrieving bucket contents from:{0}\".format(bucket_name))\n",
    "    try:\n",
    "        files = cur_conn.Bucket(bucket_name).objects.all()\n",
    "        for file in files:\n",
    "            items_list.append(\"{0}\".format(file.key))\n",
    "    except ClientError as be:\n",
    "        print(\"CLIENT ERROR: {0}\".format(be))\n",
    "    except Exception as e:\n",
    "        print(\"Unable to retrieve bucket contents: {0}\".format(e))\n",
    "    return items_list\n",
    "\n",
    "def get_item(cur_conn, bucket_name, item_name):\n",
    "    \"\"\"\n",
    "    get file contents of a particular file:\n",
    "    - based on bucket_name and item_name\n",
    "\n",
    "    \"\"\"\n",
    "    print(\n",
    "        \"Retrieving item from bucket: {0}, key: {1}\".format(\n",
    "            bucket_name, item_name\n",
    "        )\n",
    "    )\n",
    "    try:\n",
    "        file = cur_conn.Object(bucket_name, item_name).get()\n",
    "        print(\"File Contents retrieved\")\n",
    "        return file\n",
    "    except ClientError as be:\n",
    "        print(\"CLIENT ERROR: {0}\".format(be))\n",
    "    except Exception as e:\n",
    "        print(\"Unable to retrieve file contents: {0}\".format(e))\n",
    "\n",
    "def _save_doc_linux(cur_conn, bucket_name, doc_path, tmp_location):\n",
    "    \"\"\"Save file from COS to temp location locally\"\"\"\n",
    "    bytes_obj = get_item(cur_conn, bucket_name, doc_path)[\"Body\"].read()\n",
    "    with open(tmp_location, \"wb\") as f:\n",
    "        f.write(bytes_obj)\n",
    "\n",
    "\n",
    "def _replace_characters(text_replacements, init_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces characters in the init_text based on text_replacements that contains mapping {\"old_char\": \"new_char\"}.\n",
    "    Returns text with the replaced character\n",
    "    \"\"\"\n",
    "    replace_json_dict = json.loads(text_replacements)\n",
    "    output_text = init_text\n",
    "    for old_char, new_char in replace_json_dict.items():\n",
    "        output_text = output_text.replace(old_char, new_char)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "def load_split_cos_docs(\n",
    "    cur_conn,\n",
    "    bucket_name,\n",
    "    chunk_size,\n",
    "    chunk_overlap,\n",
    "    text_splitter_type,\n",
    "    text_splitter_separators,\n",
    "    text_replacements,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load and split txt, pdf and doc documents from bucket on COS, where:\n",
    "    - cur_conn is connection established to COS\n",
    "    - bucket_name is the name of the bucket with documents to load and split\n",
    "    - chunk size is the size of text to split by\n",
    "    - chunk overlap is the number of characters to overlap between chunks\n",
    "    By default it will use original language, to translate to english you need to set translate to True\n",
    "    \"\"\"\n",
    "    # list to save all docs into a list\n",
    "    split_loaded_docs = list()\n",
    "    temp_folder = os.path.join(\".\", \"temp\")\n",
    "\n",
    "    if text_splitter_separators is not None and text_splitter_separators != \"\":\n",
    "        text_splitter_separators = json.loads(text_splitter_separators, strict=False)\n",
    "\n",
    "    # location for temp files\n",
    "    bucket_contents = get_bucket_contents(cur_conn, bucket_name)\n",
    "\n",
    "    # defining the splitter\n",
    "    if text_splitter_type in text_splitter_type_dict:\n",
    "        print(f\"Using {text_splitter_type}\")\n",
    "        # free text\n",
    "        if (\n",
    "            isinstance(text_splitter_separators, list)\n",
    "            and len(text_splitter_separators) > 0\n",
    "        ):\n",
    "            text_splitter = text_splitter_type_dict[text_splitter_type](\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                separators=text_splitter_separators,\n",
    "            )\n",
    "            print(\n",
    "                f\"Specified separators for the text splitter {text_splitter_separators}\"\n",
    "            )\n",
    "        else:\n",
    "            text_splitter = text_splitter_type_dict[text_splitter_type](\n",
    "                chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "            )\n",
    "            print(\n",
    "                \"Separators for text splitters are either not specified or specified incorrectly\"\n",
    "            )\n",
    "    else:\n",
    "        print(f\"Text splitter of type {text_splitter_type} is not supported\")\n",
    "\n",
    "    # going through files\n",
    "    print(\"Looping through bucket contents\")\n",
    "    for doc_path_num, doc_path in enumerate(bucket_contents):\n",
    "        # to check only files within project sources\n",
    "        sources_folder_path = posixpath.join(os.getenv(\"COS_FOLDER\"))\n",
    "        if sources_folder_path not in doc_path:\n",
    "            continue\n",
    "        print(f\"Processing {doc_path}\")\n",
    "        cur_doc_format = doc_path.split(\".\")[-1].lower()\n",
    "        # to create temp folder if it doesn't exist\n",
    "        if not os.path.exists(temp_folder):\n",
    "            os.mkdir(temp_folder)\n",
    "        tmp_location = os.path.join(temp_folder, doc_path.split(\"/\")[-1])\n",
    "        print(f\"Attempting to load {doc_path}\")\n",
    "        # check for formatting\n",
    "        if cur_doc_format in _document_loaders_dict:\n",
    "            # to load document\n",
    "            _save_doc_linux(cur_conn, bucket_name, doc_path, tmp_location)\n",
    "            doc_loader = _document_loaders_dict[cur_doc_format](tmp_location)\n",
    "            loaded_docs = doc_loader.load()\n",
    "        else:\n",
    "            logger.warning(\n",
    "                f\"Data in {doc_path} is not supported, accepted formats are {_document_loaders_dict.keys()}\"\n",
    "            )\n",
    "            continue\n",
    "        print(f\"Successfully loaded {doc_path}\")\n",
    "\n",
    "        # to change metadata of loaded documents to include only filename for the source\n",
    "        # replace strings based on the input json\n",
    "        for loaded_doc in loaded_docs:\n",
    "            # to change metadata of loaded documents to include only filename for the source\n",
    "            loaded_doc.metadata[\"source\"] = doc_path.split(\"/\")[-1]\n",
    "            # to replace characters in page_content according to dict stored in TEXT_REPLACEMENTS\n",
    "            if text_replacements != \"\" and text_replacements is not None:\n",
    "                loaded_doc.page_content = _replace_characters(\n",
    "                    text_replacements, loaded_doc.page_content\n",
    "                )\n",
    "        # to split documents\n",
    "        print(f\"Splitting docs loaded from {doc_path}\")\n",
    "        splitted_docs = text_splitter.split_documents(loaded_docs)\n",
    "        # to add current document to total list\n",
    "        split_loaded_docs.extend(splitted_docs)\n",
    "        if os.path.exists(tmp_location):\n",
    "            os.remove(tmp_location)\n",
    "            print(f\"File {tmp_location} was removed\")\n",
    "    # after loop through COS documents ended\n",
    "    print(\"Documents from Cloud Object Storage are loaded and splitted\")\n",
    "    return {\"splitted_loaded_docs\": split_loaded_docs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f05d9950-e561-432b-a860-7de0c7cb2253"
   },
   "outputs": [],
   "source": [
    "splitted_docs = load_split_cos_docs(\n",
    "        cos_resource, INPUT_BUCKET, chunk_size, chunk_overlap, text_splitter_type, text_splitter_separators, text_replacements\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9505ae3-bd82-4602-b177-6c0dec79c9d2"
   },
   "outputs": [],
   "source": [
    "sources = []\n",
    "pages = []\n",
    "docs = []\n",
    "for splitted_doc in splitted_docs['splitted_loaded_docs']:\n",
    "    sources.append(splitted_doc.metadata['source'])\n",
    "    pages.append(splitted_doc.metadata['page'])\n",
    "    docs.append(splitted_doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb905b4e-b8c2-4ab3-8b6e-fa1345b722c9"
   },
   "outputs": [],
   "source": [
    "connections.connect(\n",
    "alias='default',\n",
    "**connection_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To drop collection if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8a6202a-7db7-410f-a55d-1c71baf7eef9"
   },
   "outputs": [],
   "source": [
    "if utility.has_collection(mv_collection): # check if collection exists\n",
    "    utility.drop_collection(mv_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To create a new collection\n",
    "> you can update schema naming to correspond to your requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12230f60-05d0-41b3-b102-dfee792b1891"
   },
   "outputs": [],
   "source": [
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"text_embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),  # Assuming 384-dim vectors\n",
    "    FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=500),\n",
    "    FieldSchema(name=\"page\", dtype=DataType.INT64),\n",
    "    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),  # Storing original content as well\n",
    "]\n",
    "schema = CollectionSchema(fields)\n",
    "collection = Collection(name=mv_collection, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa6592d3-866e-4a62-9ac2-1de5a6143bf3"
   },
   "outputs": [],
   "source": [
    "print(collection.schema.fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To insert documents into collection, create index and load data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84b4f2ae-ee88-4344-a677-7d016591c2b3"
   },
   "outputs": [],
   "source": [
    "# Convert documents into vector embeddings\n",
    "doc_embeddings = embeddings.embed_documents(docs)\n",
    "\n",
    "# Insert documents and embeddings into Milvus\n",
    "collection.insert([doc_embeddings, sources, pages, docs]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ff47033-df83-4bcf-a2bb-b8011e90efca"
   },
   "outputs": [],
   "source": [
    "# create index\n",
    "index_params = {\n",
    "    \"index_type\": \"IVF_FLAT\",  # Can also use \"IVF_PQ\", \"HNSW\", etc.\n",
    "    \"metric_type\": similarity_metric,       # L2 for Euclidean distance, or use \"IP\" for Inner Product\n",
    "    \"params\": {\"nlist\": 110},  # nlist is a hyperparameter for clustering / corresponds to cca 800 vectors\n",
    "}\n",
    "\n",
    "collection.create_index(field_name=\"text_embedding\", index_params=index_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "199b5358-cef9-48b3-ae9d-20d235c90aa4"
   },
   "outputs": [],
   "source": [
    "# Load data into memory (optional but recommended for larger datasets)\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To perform semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33187800-dbb9-44ff-b541-9c38df2cacd0"
   },
   "outputs": [],
   "source": [
    "cur_query=\"What is ETF?\"\n",
    "search_params = {\"metric_type\": similarity_metric, \"params\": {\"nprobe\": 10}}\n",
    "query_result = collection.search(\n",
    "    [embeddings.embed_query(cur_query)], \n",
    "    \"text_embedding\", \n",
    "    search_params, \n",
    "    limit=5, \n",
    "    output_fields=[\"title\", \"page\", \"text\"])\n",
    "# Print the query result\n",
    "for entity in query_result[0]:\n",
    "    print(entity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
